{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLCI Validation with *in situ* CCI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Version: 1.0\n",
    "    Date:    27/09/2019\n",
    "    Author:  Ben Loveday (Plymouth Marine Laboratory) and Hayley Evers-King (EUMETSAT)\n",
    "    Credit:  This code was developed for EUMETSAT under contracts for the Copernicus \n",
    "             programme.\n",
    "    License: This code is offered as open source and free-to-use in the public domain, \n",
    "             with no warranty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this notebook for?**\n",
    "\n",
    "This notebook will perform a validation of a series of OLCI products using the CCI *in situ* data set. If specified, it will run the matchups according to the Bailey and Werdell (2006) protocol.\n",
    "\n",
    "**What specific tools does this notebook use?**\n",
    "\n",
    "HDA_API toolkit\n",
    "insitu data toolkit\n",
    "extraction toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is divided into a series of modules that each contain a series of methods for specific tasks. The box below imports all of the moduls we need to complete our plotting task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import gridspec\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.ticker as mticker\n",
    "import json\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# specific tools (which can be found here ../../Hub_tools/)\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())) + '/Hub_Tools/')\n",
    "import harmonised_data_access_api_tools as hapi\n",
    "import insitu_tools as ist\n",
    "import extraction_tools as ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEkEO provides access to a huge number of datasets through its 'harmonised-data-access' API. This allows us to query the full data catalogue and download data quickly and directly onto our Jupyter Hub. You can search for what data is available here: https://www.wekeo.eu/dataset-navigator/start.\n",
    "\n",
    "In order to use the HDA-API we need to provide some authentication credentials, which comes in the form of an api_key. You can get your key from here; https://www.wekeo.eu/api-keys. If you click on the 'show hidden keys' button at the bottom of the page it will reveal a number of keys. The one you need is in the top grey box, and is on the following line:\n",
    "\n",
    "-H \"Authorization: Basic \"**YOUR API KEY**\"\n",
    "\n",
    "Replace \"YOUR API KEY\" below with what you copy from \"**YOUR API KEY**\" (N.B. you need to keep the quotation marks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your api key:\n",
    "api_key = \"cmJ1UGJQVzZnT09HU2RUWDJhTGFkOGY4RjhnYTpGRmFCTTNoSXluVk1NdEk4b2dPc2ZjMHFOdlVh\"\n",
    "# where the data should be downloaded to:\n",
    "download_dir_path = \"/home/jovyan/work/products\"\n",
    "# where we can find our data query form:\n",
    "JSON_query_dir = os.path.join(os.getcwd(),'JSON_templates')\n",
    "# HDA-API loud and noisy?\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will set a few parameters for selecting our data, making our plots and telling the notebook where our data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCI directory:\n",
    "CCI_directory = 'datasets'\n",
    "\n",
    "# select CCI in situ product: chla, iopskdtsm, rrs\n",
    "insitu_product = 'rrs'\n",
    "\n",
    "# satellite sensor\n",
    "sensor = 'OLCI'\n",
    "\n",
    "# use Bailey and Werdell, 2006 match-up criteria?\n",
    "BW06 = True\n",
    "\n",
    "# box size to extract around out points: use 2 for 5x5 box (Bailey and Werdell, 2006)\n",
    "sensor_box_pad_size = 2\n",
    "\n",
    "# select CCI in situ spectral resolution: 2, 6, fullrange\n",
    "insitu_resolution = '2'\n",
    "\n",
    "# test cases: set to None if launching your own analysis \n",
    "# available options for tests: Hawaii\n",
    "test_case = 'Hawaii'\n",
    "\n",
    "# subset region:\n",
    "subset_extents = [15, 30, 55, 65]\n",
    "\n",
    "# subset dates & times:\n",
    "date_start = '2016-06-01 00:00:00'\n",
    "date_end = '2019-12-31 23:59:59'\n",
    "\n",
    "# verbosity\n",
    "verbose = False\n",
    "show_plots = True\n",
    "\n",
    "# plot control\n",
    "fsz = 20\n",
    "\n",
    "# OLCI time tolerance +/- (seconds): 3 hours matches Bailey and Werdell, 2006\n",
    "time_tolerance = 3*60*60 \n",
    "x_tol = 0.001\n",
    "flags_to_use = ['CLOUD', 'CLOUD_AMBIGUOUS', 'CLOUD_MARGIN',\\\n",
    "                 'INVALID', 'COSMETIC', 'SATURATED', 'SUSPECT',\\\n",
    "                 'HISOLZEN', 'SNOW_ICE', 'AC_FAIL', 'WHITECAPS']\n",
    "flags_to_use = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement test cases if we are running one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_case == 'Hawaii':\n",
    "    # Hawaii test\n",
    "    subset_extents = [-160, -155, 20, 22.5]\n",
    "    date_start = '2018-10-10 00:00:00'\n",
    "    date_end = '2019-12-31 23:59:59'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in and do some processing on our CCI *in situ* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insitu_headers, insitu_data = ist.read_CCI_insitu_file(CCI_directory, insitu_product, insitu_resolution)\n",
    "lat_col, lon_col, time_col = ist.get_CCI_coordinate_cols(insitu_headers)\n",
    "rad_cols, lambda_cols = ist.get_CCI_radiometry_cols(insitu_headers, insitu_resolution)\n",
    "\n",
    "lats = np.asarray([float(i) for i in insitu_data[:, lat_col]])\n",
    "lons = np.asarray([float(i) for i in insitu_data[:, lon_col]])\n",
    "dates_times = np.asarray([datetime.datetime.strptime(i,\"%Y-%m-%dT%H:%M\") for i in insitu_data[:, time_col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select the relevant data for our validation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find rows that matching our subset criteria\n",
    "ii = np.where((lons >= subset_extents[0]) & (lons <= subset_extents[1])\\\n",
    "            & (lats >= subset_extents[2]) & (lats <= subset_extents[3])\\\n",
    "            & (dates_times >= datetime.datetime.strptime(date_start,\"%Y-%m-%d %H:%M:%S\"))\\\n",
    "            & (dates_times <= datetime.datetime.strptime(date_end,\"%Y-%m-%d %H:%M:%S\")))[0]\n",
    "\n",
    "insitu_data = insitu_data[ii,:]\n",
    "lats = lats[ii]\n",
    "lons = lons[ii]\n",
    "dates_times = dates_times[ii]\n",
    "print('-----------------------------')\n",
    "print(str(len(ii)) + ' rows in scope')\n",
    "print('-----------------------------')\n",
    "\n",
    "# stop if no points in scope....\n",
    "if len(ii) == 0:\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to bin the data spatially to help with plotting the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram bin the values onto a res x res grid to help adding numbers to the plot\n",
    "res = 0.25\n",
    "lon_grid = np.arange(int(subset_extents[0]),int(subset_extents[1])+1, res)\n",
    "lat_grid = np.arange(int(subset_extents[2]),int(subset_extents[3])+1, res)\n",
    "Hist, xedges, yedges = np.histogram2d(lons, lats, bins=(lon_grid, lat_grid))\n",
    "\n",
    "# a little processing\n",
    "Hist = Hist.T\n",
    "Hist[Hist==0] = np.nan\n",
    "plot_lon = (xedges[1:] + xedges[0:-1]) / 2\n",
    "plot_lat = (yedges[1:] + yedges[0:-1]) / 2\n",
    "LON, LAT = np.meshgrid(plot_lon, plot_lat)\n",
    "HIST = Hist.ravel()\n",
    "LON = LON.ravel()\n",
    "LAT = LAT.ravel()\n",
    "\n",
    "# tinkering with dates for plotting\n",
    "dates_origin = np.asarray([tt.toordinal() for tt in dates_times])\n",
    "dates_origin = (dates_origin - np.nanmin(dates_origin)) / (np.nanmax(dates_origin) - np.nanmin(dates_origin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If verbose is set to true, we will print out our selected validation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    for row in range(np.shape(insitu_data)[0]):\n",
    "        print('Lon: ' + str(insitu_data[row, lon_col])\\\n",
    "           + ' Lat: ' + str(insitu_data[row, lat_col])\\\n",
    "           + ' Time: ' + str(dates_times[row]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If show_plots is set to true we will plot our validation data. The small numbers give the number of validation on a 0.25 degree grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if show_plots:\n",
    "    # get our land mask from NaturalEarth\n",
    "    land_resolution = '10m'\n",
    "    land_poly = cfeature.NaturalEarthFeature('physical', 'land', land_resolution,\\\n",
    "                edgecolor='k',facecolor=cfeature.COLORS['land'])\n",
    "\n",
    "    # intitialise our figure\n",
    "    xsize = 20\n",
    "    ysize = xsize*(subset_extents[3] - subset_extents[2]) / (subset_extents[1] - subset_extents[0])\n",
    "    fig1 = plt.figure(figsize=(xsize, ysize), dpi=300)\n",
    "    plt.rc('font', size=fsz)\n",
    "    matplotlib.rcParams['contour.negative_linestyle'] = 'solid'\n",
    "    \n",
    "    # make an axis\n",
    "    gs = gridspec.GridSpec(3, 1, height_ratios=[20,2,1])\n",
    "    gs.update(wspace=0.01, hspace=0.01)\n",
    "    m = plt.subplot(gs[0,0], projection=ccrs.PlateCarree())\n",
    "\n",
    "    # plot the data\n",
    "    kk = np.argsort(dates_origin)\n",
    "    plot1 = m.scatter(lons[kk], lats[kk], s=(dates_origin[kk]*-1+2)*250,\n",
    "                      c=dates_origin[kk], zorder=100,\n",
    "                      cmap=plt.cm.viridis, alpha=0.75,\n",
    "                      transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # embellish with gridlines and ticks\n",
    "    m.coastlines(resolution=land_resolution, color='black', linewidth=1)\n",
    "    m.add_feature(land_poly)\n",
    "    m.set_extent(subset_extents, ccrs.PlateCarree())\n",
    "    g1 = m.gridlines(draw_labels = True, zorder=20, color='0.5', linestyle='--',linewidth=0.5)\n",
    "    g1.xlocator = mticker.FixedLocator(np.linspace(int(subset_extents[0]),\\\n",
    "                                               int(subset_extents[1]), 5))\n",
    "    g1.ylocator = mticker.FixedLocator(np.linspace(int(subset_extents[2]),\\\n",
    "                                               int(subset_extents[3]), 5))\n",
    "    g1.xlabels_top = False\n",
    "    g1.ylabels_right = False\n",
    "    g1.xlabel_style = {'size': fsz, 'color': 'black'}\n",
    "    g1.ylabel_style = {'size': fsz, 'color': 'black'}\n",
    "\n",
    "    # add annotation numbers\n",
    "    for hh in range(len(HIST)):\n",
    "        if np.isnan(HIST[hh]):\n",
    "            continue      \n",
    "        else:\n",
    "            dx = (subset_extents[1] - subset_extents[0])/80\n",
    "            dy = (subset_extents[3] - subset_extents[2])/80\n",
    "            plt.annotate(str(HIST[hh]), xy=(LON[hh]+dx, LAT[hh]+dy),\\\n",
    "                         xycoords='data', size=fsz,\\\n",
    "                         color='r', zorder=100)\n",
    "    \n",
    "    # add colorbar\n",
    "    axes0 = plt.subplot(gs[2,0])\n",
    "    cbar = plt.colorbar(plot1, cax=axes0, orientation='horizontal',ticks=[0, 1])\n",
    "    cbar.ax.set_xticklabels([min(dates_times), max(dates_times)])\n",
    "    cbar.ax.tick_params(labelsize=fsz) \n",
    "    cbar.set_label('Measurement date & time',fontsize=fsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a series of queries that check if any OLCI passes match with this data. We start by setting the data source (which we use as a key for our JSON query file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMEMS OSTIA SST KEY\n",
    "dataset_id = \"EO:EUM:DAT:SENTINEL-3_OL_2_WFR___\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our date requirements using our in situ validation points and OLCI time tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_dates = []\n",
    "end_dates = []\n",
    "for this_date in dates_times:\n",
    "    start_dates.append((this_date - datetime.timedelta(seconds=time_tolerance)).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\"))\n",
    "    end_dates.append((this_date + datetime.timedelta(seconds=time_tolerance)).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\"))\n",
    "\n",
    "if verbose:\n",
    "    for start_date, end_date in zip(start_dates, end_dates):\n",
    "        print(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our data source to load the correct JSON query file from ../JSON_templates. (This is just my convention for building queries, there are many other ways you could do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find query file\n",
    "JSON_query_file = os.path.join(JSON_query_dir,dataset_id.replace(':','_')+\".json\")\n",
    "if not os.path.exists(JSON_query_file):\n",
    "    print('Query file ' + JSON_query_file + ' does not exist')\n",
    "else:\n",
    "    print('Found JSON query file for '+dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the data. See the following scripts and notebooks for information on how this works:\n",
    "    \n",
    "    *samples/How_To_Guide-Harmonized_Data_Access-v0.1.3.ipynb*\n",
    "    *ocean-wekeo-jpyhub/HDA_API_Tools/HDA_API_downloading.ipynb*\n",
    "    *ocean-wekeo-jpyhub/Hub_Tools/harmonised_data_access_api_tools.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HAPI_dict = hapi.init(dataset_id, api_key, download_dir_path, verbose=verbose)\n",
    "HAPI_dict = hapi.get_access_token(HAPI_dict)\n",
    "HAPI_dict = hapi.accept_TandC(HAPI_dict)\n",
    "\n",
    "Downloaded_files = []\n",
    "Downloaded_iter = []\n",
    "\n",
    "count = -1\n",
    "for start_date, end_date in zip(start_dates, end_dates):\n",
    "    count = count + 1\n",
    "    print(str(int(count/len(start_dates)))+'% complete')\n",
    "    print('Running for: ' + start_date + ' to ' + end_date)\n",
    "\n",
    "    date_string = start_date.split('T')[0].replace('-','') \\\n",
    "                  + '_' + end_date.split('T')[0].replace('-','')\n",
    "    \n",
    "    # load the query\n",
    "    with open(JSON_query_file, 'r') as f:\n",
    "        query = f.read()\n",
    "        query = query.replace(\"%DATE_START%\",start_date)\n",
    "        query = query.replace(\"%DATE_END%\",end_date)\n",
    "        query = query.replace(\"%LON1%\",str(lons[count] - x_tol))\n",
    "        query = query.replace(\"%LON2%\",str(lons[count] + x_tol))\n",
    "        query = query.replace(\"%LAT1%\",str(lats[count] - x_tol))\n",
    "        query = query.replace(\"%LAT2%\",str(lats[count] + x_tol))\n",
    "        query = json.loads(query)\n",
    "    \n",
    "    # launch job\n",
    "    HAPI_dict = hapi.launch_query(HAPI_dict, query)\n",
    "\n",
    "    # wait for jobs to complete\n",
    "    HAPI_dict = hapi.check_job_status(HAPI_dict)\n",
    "    if HAPI_dict['nresults'] == 0:\n",
    "        print('Nothing to do for this query....')\n",
    "        continue\n",
    "\n",
    "    # check results and links\n",
    "    HAPI_dict = hapi.get_results_list(HAPI_dict)\n",
    "    HAPI_dict = hapi.get_download_links(HAPI_dict)\n",
    "\n",
    "    # check prospective filenames:\n",
    "    HAPI_dict = hapi.get_filenames(HAPI_dict)\n",
    "\n",
    "    # download\n",
    "    HAPI_dict = hapi.download_data(HAPI_dict, skip_existing=True)\n",
    "    \n",
    "    for file in HAPI_dict['filenames']:\n",
    "        Downloaded_files.append(file)\n",
    "        Downloaded_iter.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLCI files are downloaded in the zipped SAFE format. We have to unzip these files to use them, which we do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file\n",
    "for filename in Downloaded_files:\n",
    "    if os.path.splitext(filename)[-1] == '.zip':\n",
    "        print('Unzipping: ' + filename)\n",
    "        with ZipFile(filename, 'r') as zipObj:\n",
    "            # Extract all the contents of zip file in current directory\n",
    "            zipObj.extractall(os.path.dirname(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the downloaded files with the *in situ* record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# open relevant files and get validation points\n",
    "all_insitu_vals = []\n",
    "all_OLCI_vals = []\n",
    "all_band_wavs = []\n",
    "for ii in range(len(Downloaded_iter)):\n",
    "    input_dir = Downloaded_files[ii].replace('.zip','.SEN3')\n",
    "    band_wavs, OLCI_vals = \\\n",
    "       ext.extract_OLCI_matchup_data(input_dir,\\\n",
    "                            lats[Downloaded_iter[ii]],\\\n",
    "                            lons[Downloaded_iter[ii]],\\\n",
    "                            insitu_product,\\\n",
    "                            pad_size=sensor_box_pad_size,\\\n",
    "                            flags_to_use=flags_to_use, BW06=BW06)\n",
    "    insitu_vals = \\\n",
    "       ext.extract_CCI_matchup_data(insitu_product,\\\n",
    "                            insitu_data[Downloaded_iter[ii],:],\\\n",
    "                            rad_cols)\n",
    "        \n",
    "    # Normalise the OLCI reflectances for BDRF\n",
    "    if BW06:\n",
    "        OLCI_means = np.asarray([record['filtered_var_mean'] for record in OLCI_vals]).astype(float)\n",
    "        OLCI_CV = np.asarray([record['CV'] for record in OLCI_vals]).astype(float)\n",
    "        if np.median(OLCI_CV > 0.15):\n",
    "            OLCI_means = OLCI_means*np.nan\n",
    "    else:\n",
    "        OLCI_means = np.asarray(OLCI_vals.copy()).astype(float)\n",
    "        \n",
    "    BDRF_norm_array = OLCI_means/np.pi\n",
    "    \n",
    "    # write into list; truncating to the in situ wavelengths\n",
    "    all_insitu_vals.append(np.asarray(insitu_vals))\n",
    "    all_OLCI_vals.append(BDRF_norm_array[0:len(insitu_vals)])\n",
    "    all_band_wavs.append(np.asarray(band_wavs[0:len(insitu_vals)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the matchup data by spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set up figure\n",
    "fig1 = plt.figure(figsize=(len(all_band_wavs)*5, 10), dpi=300)\n",
    "plt.rc('font', size=fsz)\n",
    "\n",
    "# setup axes\n",
    "gs = gridspec.GridSpec(3, 1)\n",
    "gs.update(hspace=0.5)\n",
    "\n",
    "for ii in range(len(all_band_wavs)): \n",
    "    ax = plt.subplot(gs[ii,0])\n",
    "    plt.plot(all_band_wavs[ii], all_OLCI_vals[ii], color='r', linestyle='--')\n",
    "    plt.plot(all_band_wavs[ii], all_insitu_vals[ii], color='b', linestyle='--')\n",
    "    p1 = plt.scatter(all_band_wavs[ii], all_OLCI_vals[ii], color='r', s=50)\n",
    "    p2 = plt.scatter(all_band_wavs[ii], all_insitu_vals[ii], color='b', s=50)\n",
    "    plt.xlabel('Wavelength [nm]')\n",
    "    plt.ylabel('Rrs [sr$^{-1}$]')\n",
    "    plt.ylim([0, np.nanmax(all_OLCI_vals)*1.25])\n",
    "    leg = plt.legend([p1, p2],['OLCI','CCI'])\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the matchup data by wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set up panels\n",
    "ncols = 3\n",
    "nrows = (np.ceil(len(all_band_wavs[0])/ncols)).astype('int')\n",
    "\n",
    "# set up figure\n",
    "fig1 = plt.figure(figsize=(ncols*10, nrows*10), dpi=300)\n",
    "plt.rc('font', size=fsz)\n",
    "\n",
    "# set up axes\n",
    "gs = gridspec.GridSpec(nrows, ncols)\n",
    "gs.update(wspace=0.25, hspace=0.25)\n",
    "\n",
    "count = -1\n",
    "for ii in range(nrows):\n",
    "    for jj in range(ncols):\n",
    "        count = count + 1\n",
    "        if count >= len(all_band_wavs[0]):\n",
    "            continue\n",
    "        # Plot the matchups\n",
    "        ax = plt.subplot(gs[ii,jj])\n",
    "        this_wavs = [] ; this_insitu = [] ; this_olci = []\n",
    "        for kk in range((len(all_band_wavs))):\n",
    "            this_wavs.append(all_band_wavs[kk][count])\n",
    "            this_insitu.append(all_insitu_vals[kk][count])\n",
    "            this_olci.append(all_OLCI_vals[kk][count])\n",
    "        \n",
    "        this_wavs = np.asarray(this_wavs)\n",
    "        this_insitu = np.asarray(this_insitu)\n",
    "        this_olci = np.asarray(this_olci)\n",
    "        \n",
    "        p1 = plt.scatter(this_insitu, this_olci, color='r', s=50)\n",
    "        plt.xlabel('Rrs in situ [sr$^{-1}$]')\n",
    "        plt.ylabel('Rrs OLCI [sr$^{-1}$]')\n",
    "        max_val = max([np.nanmax(this_insitu), np.nanmax(this_olci)])*1.1\n",
    "        plt.xlim([0, max_val])\n",
    "        plt.ylim([0, max_val])\n",
    "        \n",
    "        # embellish\n",
    "        plt.title('Wavelength: '+str(this_wavs[0])+' nm')\n",
    "        plt.plot([0, max_val], [0, max_val], linestyle='--', color='k')\n",
    "        \n",
    "        # stats\n",
    "        slp, inc, rval, pval, stderr = st.linregress(this_insitu, y=this_olci)\n",
    "        rmse = mean_squared_error(this_insitu, this_olci)**0.5\n",
    "\n",
    "        plt.annotate('slope: {0:.4f}'.format(slp), xy=(0.05, 0.95), xycoords='axes fraction', color='b')\n",
    "        plt.annotate('intercept: {0:.4f}'.format(inc), xy=(0.05, 0.90), xycoords='axes fraction', color='b')\n",
    "        plt.annotate('rval: {0:.4f}'.format(rval), xy=(0.05, 0.85), xycoords='axes fraction', color='b')\n",
    "        plt.annotate('pval: {0:.4f}'.format(pval), xy=(0.05, 0.80), xycoords='axes fraction', color='b')\n",
    "        plt.annotate('rmse: {0:.4f}'.format(rmse), xy=(0.05, 0.75), xycoords='axes fraction', color='b')\n",
    "        plt.plot([0, max_val], [0*slp+inc, max_val*slp+inc], linestyle='--', color='r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
