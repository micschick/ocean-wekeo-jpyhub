{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Version: 1.0\n",
    "    Date:    27/09/2019\n",
    "    Author:  Ben Loveday (Plymouth Marine Laboratory) and Hayley Evers-King (EUMETSAT)\n",
    "    Credit:  This code was developed for EUMETSAT under contracts for the Copernicus \n",
    "             programme.\n",
    "    License: This code is offered as open source and free-to-use in the public domain, \n",
    "             with no warranty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this notebook for?**\n",
    "\n",
    "This notebook will download EUMETSAT Sentinel-3 data to support plotting.\n",
    "\n",
    "**What specific tools does this notebook use?**\n",
    "\n",
    "The harmonised data access API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is divided into a series of modules that each contain a series of methods for specific tasks. The box below imports all of the moduls we need to complete our plotting task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard tools\n",
    "import os, sys, json\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# specific tools (which can be found here ../Hub_tools/)\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())) + '/Hub_Tools/')\n",
    "import harmonised_data_access_api_tools as hapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEkEO provides access to a huge number of datasets through its 'harmonised-data-access' API. This allows us to query the full data catalogue and download data quickly and directly onto our Jupyter Hub. You can search for what data is available here: https://www.wekeo.eu/dataset-navigator/start.\n",
    "\n",
    "In order to use the HDA-API we need to provide some authentication credentials, which comes in the form of an api_key. You can get your key from here; https://www.wekeo.eu/api-keys. If you click on the 'show hidden keys' button at the bottom of the page it will reveal a number of keys. The one you need is in the top grey box, and is on the following line:\n",
    "\n",
    "-H \"Authorization: Basic \"**YOUR API KEY**\"\n",
    "\n",
    "Replace \"YOUR API KEY\" below with what you copy from \"**YOUR API KEY**\" (N.B. you need to keep the quotation marks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your api key:\n",
    "api_key = \"YOUR API KEY\"\n",
    "# where the data should be downloaded to:\n",
    "download_dir_path = \"/home/jovyan/work/products/Hagibis/\"\n",
    "# where we can find our data query form:\n",
    "JSON_query_dir = os.path.join(os.getcwd(),'JSON_templates')\n",
    "# HDA-API loud and noisy?\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Make our download directory if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(download_dir_path):\n",
    "    os.makedirs(download_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the data source (which we use as a key for our JSON query file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-3 keys\n",
    "dataset_ids = [\"EO:EUM:DAT:SENTINEL-3:OL_1_EFR___\",\\\n",
    "               \"EO:EUM:DAT:SENTINEL-3:OL_1_EFR___\",\\\n",
    "               \"EO:EUM:DAT:SENTINEL-3:OL_1_EFR___\",\\\n",
    "               \"EO:EUM:DAT:SENTINEL-3:OL_2_WFR___\",\\\n",
    "               \"EO:EUM:DAT:SENTINEL-3:OL_2_WFR___\"]\n",
    "\n",
    "start_dates = [\"2019-10-13T00:41:00.000Z\",\\\n",
    "               \"2019-10-12T01:07:00.000Z\",\\\n",
    "               \"2019-10-09T00:45:00.000Z\",\\\n",
    "               \"2019-10-13T00:41:00.000Z\",\\\n",
    "               \"2019-10-09T00:45:00.000Z\"]\n",
    "\n",
    "end_dates = [\"2019-10-13T00:45:00.000Z\",\\\n",
    "             \"2019-10-12T01:11:00.000Z\",\\\n",
    "             \"2019-10-09T00:50:00.000Z\",\\\n",
    "             \"2019-10-13T00:45:00.000Z\",\\\n",
    "             \"2019-10-09T00:50:00.000Z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the data. See the following scripts and notebooks for information on how this works:\n",
    "    \n",
    "    *samples/How_To_Guide-Harmonized_Data_Access-v0.1.3.ipynb*\n",
    "    *ocean-wekeo-jpyhub/HDA_API_Tools/HDA_API_downloading.ipynb*\n",
    "    *ocean-wekeo-jpyhub/Hub_Tools/harmonised_data_access_api_tools.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dataset_id, start_date, end_date in zip(dataset_ids, start_dates, end_dates):\n",
    "    # find query file\n",
    "    JSON_query_file = os.path.join(JSON_query_dir,\"EO_EUM_DAT_SENTINEL-3_GENERAL.json\")\n",
    "\n",
    "    HAPI_dict = hapi.init(dataset_id, api_key, download_dir_path, verbose=verbose)\n",
    "    HAPI_dict = hapi.get_access_token(HAPI_dict)\n",
    "    HAPI_dict = hapi.accept_TandC(HAPI_dict)\n",
    "\n",
    "    # load the query\n",
    "    with open(JSON_query_file, 'r') as f:\n",
    "        query = f.read()\n",
    "        query = query.replace(\"%DATE_START%\",start_date)\n",
    "        query = query.replace(\"%DATE_END%\",end_date)\n",
    "        query = query.replace(\"%KEY%\",dataset_id)\n",
    "        query = json.loads(query)\n",
    "\n",
    "    if verbose:\n",
    "        print(query)\n",
    "    \n",
    "    # launch job\n",
    "    HAPI_dict = hapi.launch_query(HAPI_dict, query)\n",
    "\n",
    "    # wait for jobs to complete\n",
    "    HAPI_dict = hapi.check_job_status(HAPI_dict)\n",
    "    if HAPI_dict['nresults'] == 0:\n",
    "        print('Nothing to do for this query....')\n",
    "    else:\n",
    "        HAPI_dict = hapi.get_results_list(HAPI_dict)\n",
    "        HAPI_dict = hapi.get_download_links(HAPI_dict)\n",
    "        HAPI_dict = hapi.download_data(HAPI_dict, skip_existing=True)\n",
    "    \n",
    "    # unzip file\n",
    "    for filename in HAPI_dict['filenames']:\n",
    "        if os.path.splitext(filename)[-1] == '.zip':\n",
    "            print('Unzipping file')\n",
    "            with ZipFile(filename, 'r') as zipObj:\n",
    "                # Extract all the contents of zip file in current directory\n",
    "                zipObj.extractall(os.path.dirname(filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
