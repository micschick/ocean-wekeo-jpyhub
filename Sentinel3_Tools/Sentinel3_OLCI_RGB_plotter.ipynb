{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-3 OLCI RGB plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Version: 2.0\n",
    "    Date:    17/09/2019\n",
    "    Author:  Ben Loveday (Plymouth Marine Laboratory ) and Hayley Evers-King (EUMETSAT)\n",
    "    Credit:  This code was developed for EUMETSAT under contracts for the Copernicus \n",
    "             programme.\n",
    "    License: This code is offered as open source and free-to-use in the public domain, \n",
    "             with no warranty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this notebook for?**\n",
    "\n",
    "This notebook shows you how to download an OLCI EFR (full-resolution level-1) scene using the harmonised data access API and plot it to the screen. It will walk you through some options for how handle the data, how to convert the radiometry channels to RGB, and how to re-project and plot. It also provides some tricks and tips for plotting imagery of this kind - such as how to make the image more visually appealing. Although it is developed specifically for OLCI, you could use the basis of this script for any RGB channel data (e.g. Sentinel-2 MSI, Sentinel-3 SLSTR, Sentinel-1)\n",
    "\n",
    "**What specific tools does this notebook use?**\n",
    "\n",
    "Beyond standard tools, the notebook imports some functions for managing the harmonised data access api (harmonised_data_access_api_tools.py) and some functions for helping us to plot data (image_tools.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is divided into a series of modules that each contain a series of methods for specific tasks. The box below imports all of the moduls we need to complete our plotting task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# standard tools\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import gridspec\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.ticker as mticker\n",
    "from skimage import exposure\n",
    "from IPython.core.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# specific tools (which can be found here ../Hub_tools/)\n",
    "sys.path.append(os.path.dirname(os.getcwd()) + '/Hub_Tools/')\n",
    "import harmonised_data_access_api_tools as hapi\n",
    "import image_tools as img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEkEO provides access to a huge number of datasets through its 'harmonised-data-access' API. This allows us to query the full data catalogue and download data quickly and directly onto our Jupyter Hub. You can search for what data is available here: https://www.wekeo.eu/dataset-navigator/start.\n",
    "\n",
    "In order to use the HDA-API we need to provide some authentication credentials, which comes in the form of an api_key. You can get your key from here; https://www.wekeo.eu/api-keys. If you click on the 'show hidden keys' button at the bottom of the page it will reveal a number of keys. The one you need is in the top grey box, and is on the following line:\n",
    "\n",
    "-H \"Authorization: Basic \"**YOUR API KEY**\"\n",
    "\n",
    "Replace \"YOUR API KEY\" below with what you copy from \"**YOUR API KEY**\" (N.B. you need to keep the quotation marks.)\n",
    "\n",
    "We will also define a few other parameters including where to download the data to, and if we want the HDA-API functions to be verbose. **Lastly, we will tell the notebook where to find the query we will use to find the data.** These 'JSON' queries are what we use to ask WEkEO for data. They have a very specific form, but allow us quite fine grained control over what data to get. You can find the example one that we will use here: **JSON_templates/RGB/EO_EUM_DAT_SENTINEL-3_OL_1_EFR___.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your api key:\n",
    "api_key = \"cmJ1UGJQVzZnT09HU2RUWDJhTGFkOGY4RjhnYTpGRmFCTTNoSXluVk1NdEk4b2dPc2ZjMHFOdlVh\"\n",
    "# where the data should be downloaded to:\n",
    "download_dir_path = \"/home/jovyan/work/products\"\n",
    "# where we can find our data query form:\n",
    "JSON_query_dir = os.path.join(os.getcwd(),'JSON_templates','RGB')\n",
    "# HDA-API loud and noisy?\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is done, we are going to make some choices about what we should do to our image once we have our data. There are a few operations that we are able to test with respect to images.\n",
    "\n",
    "    1. truncating the image channel extents\n",
    "        e.g. should we cut off the brightest and darkest pixels?\n",
    "        \n",
    "    2. normalising the channels as a group, or individually (which I call unhitched)\n",
    "        e.g. how do we map our red/green/intensity to values between 0 and 1\n",
    "        \n",
    "    3. histogramming the image channels to reduce the dynamic range\n",
    "        e.g. do we want to 'squash' the dynamic range of our plot to pull our certain features\n",
    "\n",
    "    4. Do we want to change the \"contrast\" and \"brightness\" (or at least rough proxies for these),  \n",
    "       across our image?\n",
    "\n",
    "You might want to test a number of options on your own specific image. This can be time consuming on large data. To make this a bit easier, there are also options for subsetting your image and re-sampling your image at coarses resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image reduction settings: resample the image every grid_factor points\n",
    "reduce_image = False\n",
    "grid_factor = 5\n",
    "\n",
    "# subset image: cut a relevant section out of an image. subset_extents [lon1,lon2,lat1,lat2] describes the section.\n",
    "subset_image = True\n",
    "subset_extents = [17.0, 22.0, 57.0, 60.0]\n",
    "\n",
    "# image truncation settings\n",
    "truncate_image = True\n",
    "min_percentile = 5\n",
    "max_percentile = 95\n",
    "\n",
    "# image normalisation settings\n",
    "unhitch = True\n",
    "channel_contrast = [1.0, 1.0, 1.0] # r,g,b\n",
    "channel_brightness = 1.0\n",
    "\n",
    "# image histogram settings\n",
    "histogram_image = True\n",
    "histogram_channels = 512\n",
    "\n",
    "# image plotting settings: e.g. fontsize (fsz)\n",
    "fsz = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have set how we want the script to run, we are ready to get some data. We start this process by telling the script what kind of data we want. In this case, this is OLCI L1 data, which has the following designation on WEkEO: **EO:EUM:DAT:SENTINEL-3:OL_1_EFR___**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLCI FULL RESOLUTION L1 FILE\n",
    "dataset_id = \"EO:EUM:DAT:SENTINEL-3:OL_1_EFR___\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use this dataset_id to find the correct, locally stored JSON query file which describes the data we want. The query file is called: **JSON_templates/RGB/EO_EUM_DAT_SENTINEL-3_OL_1_EFR___.json**\n",
    "\n",
    "You can edit this query if you want to get different data, but be aware of asking for too much data - you could be here a while. The box below gets the correct query file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find query file\n",
    "JSON_query_file = os.path.join(JSON_query_dir,dataset_id.replace(':','_')+\".json\")\n",
    "if not os.path.exists(JSON_query_file):\n",
    "    print('Query file ' + JSON_query_file + ' does not exist')\n",
    "else:\n",
    "    print('Found JSON query file for '+dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a query, we need to launch it to WEkEO to get our data. The box below takes care of this through the following steps:\n",
    "    1. initialise our HDA-API\n",
    "    2. get an access token for our data\n",
    "    3. accepts the WEkEO terms and conditions\n",
    "    4. loads our JSON query into memory\n",
    "    5. launches our search\n",
    "    6. waits for our search results\n",
    "    7. gets our result list\n",
    "    8. gets our download links\n",
    "    9. downloads our data\n",
    "\n",
    "This is quite a complex process, so much of the functionality has been buried 'behind the scenes'. If you want more information, you can check out the **harmonised_data_access_api_tools.py** python script, or the **How_To_Guide-Harmonized_Data_Access-v0.1.3.ipynb** notebook in the samples directory. The code below will report some information as it runs. At the end, it should tell you that one product has been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HAPI_dict = hapi.init(dataset_id, api_key, download_dir_path, verbose=verbose)\n",
    "HAPI_dict = hapi.get_access_token(HAPI_dict)\n",
    "HAPI_dict = hapi.accept_TandC(HAPI_dict)\n",
    "\n",
    "# load the query\n",
    "with open(JSON_query_file, 'r') as f:\n",
    "    query = json.load(f)\n",
    "\n",
    "# launch job\n",
    "HAPI_dict = hapi.launch_query(HAPI_dict, query)\n",
    "\n",
    "# wait for jobs to complete\n",
    "hapi.check_job_status(HAPI_dict)\n",
    "\n",
    "# check results\n",
    "HAPI_dict = hapi.get_results_list(HAPI_dict)\n",
    "HAPI_dict = hapi.get_download_links(HAPI_dict)\n",
    "\n",
    "# download data\n",
    "HAPI_dict = hapi.download_data(HAPI_dict, skip_existing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel data is usually distributed as a zip file, which contains the SAFE format data within. To use this, we must unzip the file. The bow below handles this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file\n",
    "for filename in HAPI_dict['filenames']:\n",
    "    if os.path.splitext(filename)[-1] == '.zip':\n",
    "        print('Unzipping file')\n",
    "        with ZipFile(filename, 'r') as zipObj:\n",
    "            # Extract all the contents of zip file in current directory\n",
    "            zipObj.extractall(os.path.dirname(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a local data file we can start to read it in. We begin by reading in the spatial grid variables (e.g. latitude and longitude). \n",
    "\n",
    "*(N.B. For OLCI, latititude and longitude are stored in a different file to the radiometry. You can find more information on the format of OLCI data by clicking on the **Sentinel-3 Marine User Handbook** via the following link: https://www.eumetsat.int/website/home/Satellites/CurrentSatellites/Sentinel3/OceanColourServices/index.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipped_file = HAPI_dict['filenames'][0].replace('.zip','.SEN3')\n",
    "ds1 = xr.open_dataset(os.path.join(unzipped_file, 'geo_coordinates.nc'))\n",
    "raster_lat = ds1.latitude.data\n",
    "raster_lon = ds1.longitude.data\n",
    "ds1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in the radiance values. To match OLCI's radiometry to what our eye sees, we need to map the radiance to channels to a red, green and blue profile that approximates what our eyes see. We do this using a mapping called 'Tristimulus' (https://www.britannica.com/science/tristimulus-system). OLCI has 21 radiance channels, but we only need the first 11 here, so lets get those..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 11\n",
    "\n",
    "if 'EFR' in unzipped_file:\n",
    "    radiometry_type = 'Oa%s_radiance'\n",
    "else:\n",
    "    radiometry_type = 'Oa%s_reflectance'\n",
    "\n",
    "for rad_channel_number in range(1, num_channels+1):\n",
    "    rad_channel = radiometry_type % (str(rad_channel_number).zfill(2))\n",
    "    rad_file = os.path.join(unzipped_file, rad_channel + '.nc') \n",
    "    rad_fid = xr.open_dataset(rad_file)\n",
    "    exec(\"Ch%s = rad_fid.%s.data\" % (str(rad_channel_number).zfill(2),rad_channel))\n",
    "    rad_fid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the Tristimulus coefficients for OLCI to map the radiances to red, green and blue channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = np.log10(1.0 + 0.01 * Ch01 + 0.09 * Ch02 + 0.35 * Ch03 + 0.04 * Ch04 + 0.01 * Ch05 + 0.59 * Ch06 + 0.85 * Ch07 + 0.12 * Ch08 + 0.07 * Ch09 + 0.04 * Ch10)\n",
    "green = np.log10(1.0 + 0.26 * Ch03 + 0.21 * Ch04 + 0.50 * Ch05 + Ch06 + 0.38 * Ch07 + 0.04 * Ch08 + 0.03 * Ch09 + 0.02 * Ch10)\n",
    "blue = np.log10(1.0 + 0.07 * Ch01 + 0.28 * Ch02 + 1.77 * Ch03 + 0.47 * Ch04 + 0.16 * Ch05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our RGB channels, we can manipulate them for the sake of plotting. The boxes below will run **ONLY** if you set the required tag to **True** above. Subset and reduce have been described above. Truncate_image will, by default find the pixels that are darker/lighter than 5%/95% of the image and set them to the 5%/95% value. This stops very bright/dark pixels from dominating our colour range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subset_image:\n",
    "    i1, i2, j1, j2 = img.subset_image(raster_lat, raster_lon, subset_extents)\n",
    "    raster_lat = raster_lat[i1:i2,j1:j2]\n",
    "    raster_lon = raster_lon[i1:i2,j1:j2]\n",
    "    red = red[i1:i2,j1:j2]\n",
    "    green = green[i1:i2,j1:j2]\n",
    "    blue = blue[i1:i2,j1:j2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reduce_image:\n",
    "    raster_lat = img.reduce_image(raster_lat, grid_factor=grid_factor)\n",
    "    raster_lon = img.reduce_image(raster_lon, grid_factor=grid_factor)\n",
    "    red = img.reduce_image(red, grid_factor=grid_factor)\n",
    "    green = img.reduce_image(green, grid_factor=grid_factor)\n",
    "    blue = img.reduce_image(blue, grid_factor=grid_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if truncate_image:\n",
    "    red = img.truncate_image(red)\n",
    "    green = img.truncate_image(green)\n",
    "    blue = img.truncate_image(blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, we are going to \"stack\" our RGB channels into a single image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = np.shape(red)[0]\n",
    "width = np.shape(red)[1]\n",
    "image_array = np.zeros((height, width, 3), dtype=np.float32)\n",
    "\n",
    "image_array[..., 0] = red\n",
    "image_array[..., 1] = green\n",
    "image_array[..., 2] = blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalise the image. We have to do this so ensure that we can map the luminosity values for each channel to values between 0 and 1 so python can map the numbers to a colour. However, we can do this either by separating the channels (unhitch) or by considering all channels together. By unhitching, we can underplay the dominance of the blue channel in L1 products. *N.B. we are really starting to drift away from 'true colour' now.* The box below will normalise our image array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = img.norm_image(image_array, contrast=channel_contrast, unhitch=unhitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply a histogram to the image, which may improve our image a little more. *N.B. Be aware that older version of skimage may return errors at this point; you may need to upgrade.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if histogram_image:\n",
    "    image_array = exposure.equalize_adapthist(image_array, nbins=histogram_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to map our image to a colour array which we will use to plot our scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_rgb = image_array[:, :-1, :]\n",
    "colorTuple = mesh_rgb.reshape((mesh_rgb.shape[0] * mesh_rgb.shape[1]), 3)\n",
    "colorTuple = np.insert(colorTuple, 3, 1.0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to just plot an image, without any georeferencing or mapping, we can do this from here using plt.imshow(). But our goal is to add mapping etc., so instead we are going to use plt.pcolormesh(), which we can geolocate on a pixel-by-pixel basis. \n",
    "\n",
    "However, If we try to plot using the native projection it becomes problematic as our pixels are not regularly shaped. This can result in white line artefacts in our image. To avoid this problem, we reproject the data to a more regular projection. Here we use the Mercator projection which, even though it is not ideal, is currently the only projection apart form platecaree (lat/lon) that supports gridlines in cartopy (our mapping toolkit).\n",
    "\n",
    "The box below will take care of all the plotting. There are a great many options to set here, so please have a play and see what you can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get our land mask from NaturalEarth\n",
    "land_resolution = '10m'\n",
    "land_poly = cfeature.NaturalEarthFeature('physical', 'land', land_resolution,\n",
    "                                    edgecolor='k',\n",
    "                                    facecolor=cfeature.COLORS['land'])\n",
    "\n",
    "# intitialise our figure\n",
    "fig1 = plt.figure(figsize=(20, 20), dpi=300)\n",
    "plt.rc('font', size=fsz)\n",
    "matplotlib.rcParams['contour.negative_linestyle'] = 'solid'\n",
    "\n",
    "# make an axis\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "m = plt.subplot(gs[0,0], projection=ccrs.Mercator())\n",
    "\n",
    "# plot the data\n",
    "plot1 = m.pcolormesh(raster_lon, raster_lat, \\\n",
    "                     red * np.nan, color=colorTuple ** channel_brightness, \\\n",
    "                     clip_on = True,\n",
    "                     edgecolors=None, zorder=0, \\\n",
    "                     transform=ccrs.PlateCarree())\n",
    "\n",
    "# change the plot extent if required\n",
    "if subset_image:\n",
    "    m.set_extent(subset_extents, crs=ccrs.PlateCarree())\n",
    "\n",
    "# embellish with gridlines and ticks\n",
    "g1 = m.gridlines(draw_labels = True, zorder=20, color='0.5', linestyle='--',linewidth=0.5)\n",
    "g1.xlocator = mticker.FixedLocator(np.linspace(int(subset_extents[0]),\\\n",
    "                                               int(subset_extents[1]), 5))\n",
    "g1.ylocator = mticker.FixedLocator(np.linspace(int(subset_extents[2]),\\\n",
    "                                               int(subset_extents[3]), 5))\n",
    "g1.xlabels_top = False\n",
    "g1.ylabels_right = False\n",
    "g1.xlabel_style = {'size': fsz, 'color': 'black'}\n",
    "g1.ylabel_style = {'size': fsz, 'color': 'black'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a bit of luck, you now have a wonderful image of a cyanobacterial bloom in the Baltic Sea! You can find more information on this image here: https://www.eumetsat.int/website/home/Images/ImageLibrary/DAT_4574832.html.\n",
    "\n",
    "If you like, now try to run this script on any other OLCI L1 products, or adapt it for other products. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
